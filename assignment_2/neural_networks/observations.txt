General Observations:

	Very small batch size as well as very large batch size affects the
	convergence of the neural network in a negative way. It should be kept
	at some value in between. I used a batch size of 50 for all the cases.

	Usually, a fully connected layer should not have more more neurons
	than the fully connected layer before it. This is what I observed.
	Eg. 784(input) -> 50 -> 20 -> 10 gives better accuracy than
		784(input) -> 20 -> 50 -> 10.

	I also observed that sometimes, increasing the number of layers in
	Tasks 2.1 and 2.2 can result in a decrease in accuracy.

FOR ALL TASKS I HAVE CHECKED ON SEED VALUES OF 1,10,100,1000.

TASK 2.1

For Squares dataset, I observed that the minimal network required
to pass the required accuracy of 90% is:
nn1 = nn.NeuralNetwork(2, 0.2, 50, 20)
nn1.addLayer(FullyConnectedLayer(2, 4))
nn1.addLayer(FullyConnectedLayer(4, 2))
Even though I have trained for 20 epochs, the 90% mark is crossed after
7 epochs only.
Also, keeping the hidden layer of 3 neurons also crosses 90% in most
cases. BUT I observed that on some seed values the accuracy is around 85%.
So, to be safe for all seed values, I have used 4 neurons in the hidden layer.
We can not use 1 neuron in the hidden layer as the input to that layer is not
linearly separable.

TASK 2.2

For Semi-Circle dataset, I observed that the minimal network required
to pass the required accuracy of 90% is:
nn1 = nn.NeuralNetwork(2, 0.2, 50, 20)
nn1.addLayer(FullyConnectedLayer(2, 2))
nn1.addLayer(FullyConnectedLayer(2, 2))
Even though I have trained for 20 epochs, the 90% mark is crossed after
the first epoch only.
Task 2.2 needs lesser neurons as the dataset can be separated using
linear lines more easily as compared to the XOR dataset.

TASK 2.3

For MNIST dataset, I observed that the minimal network required
to pass the required accuracy of 90% is:
nn1 = nn.NeuralNetwork(10, 0.05, 20, 20)
nn1.addLayer(FullyConnectedLayer(784, 10))
The accuracy remains very close to 90%. I checked on various seed values,
and was getting test accuracy of around 91%.
HOWEVER, there might be a seed value in which the accuracy is slightly
less than 90%. The fact that only one layer gives the accuracy of over
90% shows that the data is linearly separable.

TASK 2.4

For CIFAR Dataset, this is one of the networks that performs well, i.e.
gives an accuracy greater than 35%.
nn1 = nn.NeuralNetwork(10, 0.1, 20, 20)
nn1.addLayer(ConvolutionLayer((3,32,32), (10,10), 4, 2))
nn1.addLayer(AvgPoolingLayer((4,12,12), (2,2), 2))
nn1.addLayer(FlattenLayer())
nn1.addLayer(FullyConnectedLayer(144, 50))
nn1.addLayer(FullyConnectedLayer(50, 10))
It gives accuracy of 39.5% with a seed value of 1.

Also, the following network gives accuracy of 39% with a seed value of 10.
nn1 = nn.NeuralNetwork(10, 0.1, 20, 20)
nn1.addLayer(ConvolutionLayer((3,32,32), (10,10), 4, 2))
nn1.addLayer(AvgPoolingLayer((4,12,12), (2,2), 2))
nn1.addLayer(FlattenLayer())
nn1.addLayer(FullyConnectedLayer(144, 10))
But val accuracy is around 36% so it might be sensitive to seed value and can
go to accuracy lower than 35%.
	